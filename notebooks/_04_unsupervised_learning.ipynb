{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c6e7656",
   "metadata": {},
   "source": [
    "### ***`Unsupervised Machine Learning`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0491d83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from functools import lru_cache\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e21da37",
   "metadata": {},
   "source": [
    "##### `Reading modelpoints dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "732cb585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads model points data from local directory\n",
    "def loading_modelpoint_data(file_name: str) -> pd.DataFrame:\n",
    "    path = (Path('.').cwd().parent / 'data/processed') / file_name\n",
    "    df = pd.read_parquet(path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d899a7e",
   "metadata": {},
   "source": [
    "##### `Split data modelpoints`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7860e81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>target</th>\n",
       "      <th>item_id</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>segment</th>\n",
       "      <th>beh_segment</th>\n",
       "      <th>total_user_interaction</th>\n",
       "      <th>total_user_interation_per_item</th>\n",
       "      <th>item_popularity</th>\n",
       "      <th>normalized_popularity</th>\n",
       "      <th>...</th>\n",
       "      <th>item_type_lifestyle</th>\n",
       "      <th>item_type_transact</th>\n",
       "      <th>active_mode_active</th>\n",
       "      <th>active_mode_cold start</th>\n",
       "      <th>active_mode_semi active</th>\n",
       "      <th>screen_page_screen1</th>\n",
       "      <th>screen_page_screen2</th>\n",
       "      <th>time_action_trans(days)</th>\n",
       "      <th>trans_probability</th>\n",
       "      <th>transition_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28951</td>\n",
       "      <td>checkout</td>\n",
       "      <td>ctln</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>52709</td>\n",
       "      <td>9.469817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28951</td>\n",
       "      <td>checkout</td>\n",
       "      <td>ctln</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>52709</td>\n",
       "      <td>9.469817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28951</td>\n",
       "      <td>checkout</td>\n",
       "      <td>ctln</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>52709</td>\n",
       "      <td>9.469817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28951</td>\n",
       "      <td>checkout</td>\n",
       "      <td>ctln</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>52709</td>\n",
       "      <td>9.469817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28951</td>\n",
       "      <td>checkout</td>\n",
       "      <td>ctln</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>52709</td>\n",
       "      <td>9.469817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id    target item_id  time_of_day  segment  beh_segment  \\\n",
       "0    28951  checkout    ctln            1        0            0   \n",
       "1    28951  checkout    ctln            1        0            0   \n",
       "2    28951  checkout    ctln            1        0            0   \n",
       "3    28951  checkout    ctln            1        0            0   \n",
       "4    28951  checkout    ctln            1        0            0   \n",
       "\n",
       "   total_user_interaction  total_user_interation_per_item  item_popularity  \\\n",
       "0                      18                               6            52709   \n",
       "1                      18                               6            52709   \n",
       "2                      18                               6            52709   \n",
       "3                      18                               6            52709   \n",
       "4                      18                               6            52709   \n",
       "\n",
       "   normalized_popularity  ...  item_type_lifestyle  item_type_transact  \\\n",
       "0               9.469817  ...                  0.0                 0.0   \n",
       "1               9.469817  ...                  0.0                 0.0   \n",
       "2               9.469817  ...                  0.0                 0.0   \n",
       "3               9.469817  ...                  0.0                 0.0   \n",
       "4               9.469817  ...                  0.0                 0.0   \n",
       "\n",
       "   active_mode_active  active_mode_cold start  active_mode_semi active  \\\n",
       "0                 0.0                     0.0                      1.0   \n",
       "1                 0.0                     0.0                      1.0   \n",
       "2                 0.0                     0.0                      1.0   \n",
       "3                 0.0                     0.0                      1.0   \n",
       "4                 0.0                     0.0                      1.0   \n",
       "\n",
       "   screen_page_screen1  screen_page_screen2  time_action_trans(days)  \\\n",
       "0                  1.0                  0.0                       12   \n",
       "1                  1.0                  0.0                       12   \n",
       "2                  1.0                  0.0                        0   \n",
       "3                  1.0                  0.0                        0   \n",
       "4                  1.0                  0.0                        0   \n",
       "\n",
       "   trans_probability  transition_category  \n",
       "0                0.5                    0  \n",
       "1                0.5                    0  \n",
       "2                0.5                    1  \n",
       "3                0.5                    1  \n",
       "4                0.5                    1  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading Modepoints\n",
    "modelpoint_train = loading_modelpoint_data('modelpoint_train.parquet')\n",
    "modelpoint_train = modelpoint_train[modelpoint_train.item_id != 'no_item_id']\n",
    "modelpoint_eval = loading_modelpoint_data('modelpoint_eval.parquet')\n",
    "modelpoint_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94e15b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_points = modelpoint_train.copy()\n",
    "\n",
    "y = X_points['target']\n",
    "X = X_points.drop(columns=['target', 'item_id', 'user_id']).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e498c0",
   "metadata": {},
   "source": [
    "### **`Clustering Experiments`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fed4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import joblib\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Union, Dict, Any\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    davies_bouldin_score,\n",
    "    calinski_harabasz_score\n",
    ")\n",
    "\n",
    "# Logging Setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd52a8b",
   "metadata": {},
   "source": [
    "#### `Baseline implementation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e9fec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering models: baseline implementation\n",
    "@dataclass\n",
    "class EvaluationMetric:\n",
    "    silhouette: float\n",
    "    davies_bouldin: float\n",
    "    calinski_harabasz: float\n",
    "\n",
    "@dataclass\n",
    "class ModelInfo:\n",
    "    name: str\n",
    "    params: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    @classmethod\n",
    "    def from_kwargs(cls, name: str, **kwargs):\n",
    "        return cls(name=name, params=kwargs)\n",
    "\n",
    "@dataclass\n",
    "class ClusteringMetadata:\n",
    "    version: float\n",
    "    data_size: int\n",
    "    n_clusters: int\n",
    "    model: ModelInfo\n",
    "    metrics: EvaluationMetric\n",
    "\n",
    "@dataclass\n",
    "class ClusterModel:\n",
    "    data: Union[pd.DataFrame, np.ndarray]\n",
    "    data_size: int\n",
    "    version: float = 1.0\n",
    "    verbose: bool = False\n",
    "\n",
    "    def _save_metadata_to_local(\n",
    "        self,\n",
    "        model: BaseEstimator,\n",
    "        metadata: ClusteringMetadata,\n",
    "        cluster_name: str\n",
    "    ) -> None:\n",
    "        file_directory = Path.cwd().parent / \"models/clusters\"\n",
    "        file_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        model_path = file_directory / f\"{cluster_name}_model_v{self.version}.pkl\"\n",
    "        meta_path = file_directory / f\"{cluster_name}_metadata_v{self.version}.json\"\n",
    "\n",
    "        joblib.dump(model, model_path)\n",
    "        with open(meta_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(asdict(metadata), f, indent=4)\n",
    "\n",
    "        logger.info(f\"Saved model to: {model_path}\")\n",
    "        logger.info(f\"Saved metadata to: {meta_path}\")\n",
    "\n",
    "        if self.verbose:\n",
    "            print(json.dumps(asdict(metadata), indent=2))\n",
    "\n",
    "@dataclass            \n",
    "class GMM_ClusterModel(ClusterModel):\n",
    "    n_comp: int = 3\n",
    "    cov_type: str = 'full'\n",
    "    seed: int = 43\n",
    "    cluster_name: str = 'gaussian-mixture'\n",
    "    \n",
    "    def train_model(self) -> None:\n",
    "        logger.info(f\"Training model: [{self.cluster_name}] with {self.n_comp} components\")\n",
    "\n",
    "        model = GaussianMixture(\n",
    "            n_components=self.n_comp,\n",
    "            covariance_type=self.cov_type,\n",
    "            random_state=self.seed\n",
    "        )\n",
    "\n",
    "        X = self.data.values if isinstance(self.data, pd.DataFrame) else self.data\n",
    "        labels = model.fit_predict(X)\n",
    "        n_clusters = len(np.unique(labels))\n",
    "\n",
    "        metrics = EvaluationMetric(\n",
    "            silhouette=float(silhouette_score(X, labels)),\n",
    "            davies_bouldin=float(davies_bouldin_score(X, labels)),\n",
    "            calinski_harabasz=float(calinski_harabasz_score(X, labels))\n",
    "        )\n",
    "\n",
    "        metadata = ClusteringMetadata(\n",
    "            version=self.version,\n",
    "            data_size=self.data_size,\n",
    "            n_clusters=n_clusters,\n",
    "            model=ModelInfo.from_kwargs(\n",
    "                name=self.cluster_name,\n",
    "                n_components=self.n_comp,\n",
    "                seed=self.seed,\n",
    "                covariance_type=self.cov_type\n",
    "            ),\n",
    "            metrics=metrics\n",
    "        )\n",
    "\n",
    "        self._save_metadata_to_local(model, metadata, self.cluster_name)\n",
    "\n",
    "@dataclass            \n",
    "class DBSCAN_ClusterModel(ClusterModel):\n",
    "    eps: float = 0.3\n",
    "    min_samples: int = 5\n",
    "    seed: int = 43\n",
    "    cluster_name: str = 'dbscan'\n",
    "    \n",
    "    def train_model(self) -> None:\n",
    "        logger.info(f\"Training model: [{self.cluster_name}] with eps={self.eps}, min_samples={self.min_samples}\")\n",
    "\n",
    "        model = DBSCAN(eps=self.eps, min_samples=self.min_samples)\n",
    "\n",
    "        X = self.data.values if isinstance(self.data, pd.DataFrame) else self.data\n",
    "        labels = model.fit_predict(X)\n",
    "        n_clusters = len(np.unique(labels)) - (1 if -1 in labels else 0)\n",
    "        \n",
    "        mask = labels != -1\n",
    "        if np.sum(mask) > 1 and len(set(labels[mask])) > 1:\n",
    "            sil = silhouette_score(X[mask], labels[mask])\n",
    "            db = davies_bouldin_score(X[mask], labels[mask])\n",
    "            ch = calinski_harabasz_score(X[mask], labels[mask])\n",
    "        else:\n",
    "            sil = db = ch = float('nan')\n",
    "            \n",
    "        metrics = EvaluationMetric(\n",
    "            silhouette=float(sil),\n",
    "            davies_bouldin=float(db),\n",
    "            calinski_harabasz=float(ch)\n",
    "        )\n",
    "\n",
    "        metadata = ClusteringMetadata(\n",
    "            version=self.version,\n",
    "            data_size=self.data_size,\n",
    "            n_clusters=n_clusters,\n",
    "            model=ModelInfo.from_kwargs(\n",
    "                name=self.cluster_name,\n",
    "                eps=self.eps,\n",
    "                min_samples=self.min_samples,\n",
    "                seed=self.seed\n",
    "            ),\n",
    "            metrics=metrics\n",
    "        )\n",
    "\n",
    "        self._save_metadata_to_local(model, metadata, self.cluster_name)\n",
    "\n",
    "@dataclass            \n",
    "class Kmeans_ClusterModel(ClusterModel):\n",
    "    n_cluster: int = 5\n",
    "    seed: int = 43\n",
    "    cluster_name: str = 'k-means'\n",
    "    \n",
    "    def train_model(self) -> None:\n",
    "        logger.info(f\"Training model: [{self.cluster_name}] with n_clusters={self.n_cluster}\")\n",
    "\n",
    "        model = KMeans(n_clusters=self.n_cluster, random_state=self.seed)\n",
    "\n",
    "        X = self.data.values if isinstance(self.data, pd.DataFrame) else self.data\n",
    "        labels = model.fit_predict(X)\n",
    "        n_clusters = len(np.unique(labels))\n",
    "        \n",
    "        metrics = EvaluationMetric(\n",
    "            silhouette=float(silhouette_score(X, labels)),\n",
    "            davies_bouldin=float(davies_bouldin_score(X, labels)),\n",
    "            calinski_harabasz=float(calinski_harabasz_score(X, labels))\n",
    "        )\n",
    "        \n",
    "        metadata = ClusteringMetadata(\n",
    "            version=self.version,\n",
    "            data_size=self.data_size,\n",
    "            n_clusters=n_clusters,\n",
    "            model=ModelInfo.from_kwargs(\n",
    "                name=self.cluster_name,\n",
    "                n_clusters=self.n_cluster,\n",
    "                seed=self.seed\n",
    "            ),\n",
    "            metrics=metrics\n",
    "        )\n",
    "\n",
    "        self._save_metadata_to_local(model, metadata, self.cluster_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edae7804",
   "metadata": {},
   "source": [
    "`Clustering Experiments`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8a346f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_size = 5000\n",
    "x_training = X.copy()\n",
    "x_training = x_training.iloc[:_size]\n",
    "row_size = len(x_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b386faf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 08:40:17,368 - INFO - Training model: [gaussian-mixture] with 3 components\n",
      "2025-05-31 08:40:24,760 - INFO - Saved model to: /mnt/d/research-workspace/workx-projects/fnb-data-quest-v1/models/clusters/gaussian-mixture_model_v1.0.pkl\n",
      "2025-05-31 08:40:24,762 - INFO - Saved metadata to: /mnt/d/research-workspace/workx-projects/fnb-data-quest-v1/models/clusters/gaussian-mixture_metadata_v1.0.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"version\": 1.0,\n",
      "  \"data_size\": 5000,\n",
      "  \"n_clusters\": 3,\n",
      "  \"model\": {\n",
      "    \"name\": \"gaussian-mixture\",\n",
      "    \"params\": {\n",
      "      \"n_components\": 3,\n",
      "      \"seed\": 43,\n",
      "      \"covariance_type\": \"full\"\n",
      "    }\n",
      "  },\n",
      "  \"metrics\": {\n",
      "    \"silhouette\": 0.5374587944228525,\n",
      "    \"davies_bouldin\": 0.5536497404230104,\n",
      "    \"calinski_harabasz\": 20163.18579328827\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "cluster = GMM_ClusterModel(x_training, row_size, 1.0, verbose=True)\n",
    "cluster.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6999cf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 08:41:19,843 - INFO - Training model: [dbscan] with eps=0.3, min_samples=5\n",
      "2025-05-31 08:41:20,493 - INFO - Saved model to: /mnt/d/research-workspace/workx-projects/fnb-data-quest-v1/models/clusters/dbscan_model_v1.0.pkl\n",
      "2025-05-31 08:41:20,494 - INFO - Saved metadata to: /mnt/d/research-workspace/workx-projects/fnb-data-quest-v1/models/clusters/dbscan_metadata_v1.0.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"version\": 1.0,\n",
      "  \"data_size\": 5000,\n",
      "  \"n_clusters\": 184,\n",
      "  \"model\": {\n",
      "    \"name\": \"dbscan\",\n",
      "    \"params\": {\n",
      "      \"eps\": 0.3,\n",
      "      \"min_samples\": 5,\n",
      "      \"seed\": 43\n",
      "    }\n",
      "  },\n",
      "  \"metrics\": {\n",
      "    \"silhouette\": 0.997201879386879,\n",
      "    \"davies_bouldin\": 0.006046612848574331,\n",
      "    \"calinski_harabasz\": 3419161830694.2065\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dbscan_cluster = DBSCAN_ClusterModel(x_training, row_size, 1.0, verbose=True)\n",
    "dbscan_cluster.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b757eef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 08:42:24,725 - INFO - Training model: [k-means] with n_clusters=5\n",
      "2025-05-31 08:42:25,370 - INFO - Saved model to: /mnt/d/research-workspace/workx-projects/fnb-data-quest-v1/models/clusters/k-means_model_v1.0.pkl\n",
      "2025-05-31 08:42:25,371 - INFO - Saved metadata to: /mnt/d/research-workspace/workx-projects/fnb-data-quest-v1/models/clusters/k-means_metadata_v1.0.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"version\": 1.0,\n",
      "  \"data_size\": 5000,\n",
      "  \"n_clusters\": 5,\n",
      "  \"model\": {\n",
      "    \"name\": \"k-means\",\n",
      "    \"params\": {\n",
      "      \"n_clusters\": 5,\n",
      "      \"seed\": 43\n",
      "    }\n",
      "  },\n",
      "  \"metrics\": {\n",
      "    \"silhouette\": 0.7042568273525505,\n",
      "    \"davies_bouldin\": 0.32180743749555596,\n",
      "    \"calinski_harabasz\": 88067.9334157285\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "kmeans_cluster = Kmeans_ClusterModel(x_training, row_size, 1.0, verbose=True)\n",
    "kmeans_cluster.train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512eef2a",
   "metadata": {},
   "source": [
    "#### ***`Optimizing Clustering Techniques`***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e3e8c1",
   "metadata": {},
   "source": [
    "`DBSCAN Parameter-Tuning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af6c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "def dbscan_grid_scores(X, eps_values, min_samples_values):\n",
    "    results = []\n",
    "\n",
    "    for eps in eps_values:\n",
    "        for min_samples in min_samples_values:\n",
    "            db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            labels = db.fit_predict(X)\n",
    "            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "            if n_clusters > 1:\n",
    "                mask = labels != -1\n",
    "                sil_score = silhouette_score(X[mask], labels[mask])\n",
    "                db_score = davies_bouldin_score(X[mask], labels[mask])\n",
    "                ch_score = calinski_harabasz_score(X[mask], labels[mask])\n",
    "            else:\n",
    "                sil_score, db_score, ch_score = np.nan, np.nan, np.nan\n",
    "\n",
    "            results.append({\n",
    "                'eps': eps,\n",
    "                'min_samples': min_samples,\n",
    "                'n_clusters': n_clusters,\n",
    "                'silhouette': sil_score,\n",
    "                'db_index': db_score,\n",
    "                'calinski_harabasz': ch_score\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85f0bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameter Scores\n",
    "eps_range = np.round(np.linspace(0.3, 2.0, 3), 2)\n",
    "min_samples_range = [3, 5]\n",
    "# eps_range = np.round(np.linspace(0.3, 2.0, 5), 2)\n",
    "# min_samples_range = [3, 5, 8, 10, 15]\n",
    "dbscan_score_df = dbscan_grid_scores(X_dense, eps_range, min_samples_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97b4929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for DBSCAN parameters\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "def plot_metric_heatmap(df, metric, sci_notation=False):\n",
    "    pivot_table = df.pivot(index=\"eps\", columns=\"min_samples\", values=metric)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    fmt = \".2e\" if sci_notation else \".2f\"\n",
    "\n",
    "    ax = sns.heatmap(\n",
    "        pivot_table,\n",
    "        annot=True,\n",
    "        fmt=fmt,\n",
    "        cmap=\"YlGnBu\",\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={'format': FuncFormatter(lambda x, _: f'{x:.0e}') if sci_notation else None}\n",
    "    )\n",
    "\n",
    "    plt.title(f\"DBSCAN: {metric} Score\")\n",
    "    plt.xlabel(\"min_samples\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784e21c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_heatmap(dbscan_score_df, \"silhouette\")\n",
    "plot_metric_heatmap(dbscan_score_df, \"db_index\")\n",
    "plot_metric_heatmap(dbscan_score_df, \"calinski_harabasz\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa8a960",
   "metadata": {},
   "source": [
    "##### *Kmeans Paremter-Tuning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf6e52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Elbow Method to determine the optimal number of clusters\n",
    "def elbow_method_kmeans(X, min_value=2, max_value=10):\n",
    "    inertias = []\n",
    "    silhouette_scores = []\n",
    "    k_range = range(min_value, max_value + 1)\n",
    "    \n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(X)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        # Silhouette score is only defined for k > 1\n",
    "        if k > 1:\n",
    "            score = silhouette_score(X_dense, kmeans.labels_)\n",
    "            silhouette_scores.append(score)\n",
    "        else:\n",
    "            silhouette_scores.append(None)\n",
    "\n",
    "    # Plot Inertia vs. K (Elbow Method)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.style.use('ggplot')\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(k_range, inertias, marker='o')\n",
    "    plt.title(\"Elbow Method: Inertia vs. K\")\n",
    "    plt.xlabel(\"Number of Clusters (K)\")\n",
    "    plt.ylabel(\"Inertia\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot Silhouette Score vs. K\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(k_range[1:], silhouette_scores[1:], marker='o', color='green')\n",
    "    plt.title(\"Silhouette Score vs. K\")\n",
    "    plt.xlabel(\"Number of Clusters (K)\")\n",
    "    plt.ylabel(\"Silhouette Score\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fadc911",
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_method_kmeans(X_dense, min_value=1, max_value=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbde3ce8",
   "metadata": {},
   "source": [
    "##### *GMM Clustering*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef6b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Scores Across n_components\n",
    "n_components_range = range(2, 8)  # Try 2 to 10 clusters\n",
    "gmm_results = evaluate_gmm(X_dense, n_components_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7461294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_gmm_scores(results_df):\n",
    "    plt.figure(figsize=(14, 4))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.lineplot(x='n_components', y='silhouette', data=results_df, marker='o')\n",
    "    plt.title('Silhouette Score')\n",
    "    plt.xlabel('n_components')\n",
    "    plt.ylabel('Score')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.lineplot(x='n_components', y='db_index', data=results_df, marker='o')\n",
    "    plt.title('Davies-Bouldin Index')\n",
    "    plt.xlabel('n_components')\n",
    "    plt.ylabel('Score (lower is better)')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.lineplot(x='n_components', y='calinski_harabasz', data=results_df, marker='o')\n",
    "    plt.title('Calinski-Harabasz Index')\n",
    "    plt.xlabel('n_components')\n",
    "    plt.ylabel('Score (higher is better)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8427a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gmm_scores(gmm_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f4afef",
   "metadata": {},
   "source": [
    "#### ***`Hybrid RecoSys`***: Supervised Learning + Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d868243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Personalized Recommendation system\n",
    "def personalized_recosys_(data: pd.DataFrame, user_id: int, predicted_target: str, predicted_cluster: int) -> pd.DataFrame:\n",
    "    # Group and count item frequency by cluster, target, and item\n",
    "    group_recosys = (\n",
    "        data\n",
    "        .groupby(['gmm_cluser', 'target', 'item_id'])\n",
    "        .size()\n",
    "        .reset_index(name='ranked_similarity')\n",
    "    )\n",
    "\n",
    "    # Filter for the user's predicted cluster and target\n",
    "    items = group_recosys[\n",
    "        (group_recosys['gmm_cluser'] == predicted_cluster) &\n",
    "        (group_recosys['target'] == predicted_target)\n",
    "    ].sort_values(by='ranked_similarity', ascending=False)\n",
    "    print(items)\n",
    "    # Add user_id column and return top 3 items\n",
    "    top_ranked_items = items[['item_id', 'ranked_similarity']]\n",
    "    # top_ranked_items.insert(0, 'user_id', user_id)\n",
    "    return top_ranked_items.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd05fcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised Learning Model to predict user interaction (click or checkout)\n",
    "file_name_model = Path('.').cwd().parent / 'models/classifiers/xgbclassifier_model_v0.1.pkl'\n",
    "xgb_classifier = joblib.load(file_name_model)\n",
    "x_val = modelpoint_eval.iloc[99:100]\n",
    "predicted_val = xgb_classifier.predict(x_val.drop(['user_id', 'target','item_id'], axis=1))\n",
    "predicted_int = le.inverse_transform(predicted_val)[0]\n",
    "predicted_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950c574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_labels = gm_cluser_model['model'].predict(data_cluster)\n",
    "data_ = data_cluster1.copy()\n",
    "data_['gmm_cluser'] = gmm_labels\n",
    "\n",
    "\n",
    "user_id = 14454\n",
    "predicted_target = 'checkout'\n",
    "predicted_cluster = 1\n",
    "personalized_recosys_(data_, user_id, predicted_target, predicted_cluster)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
